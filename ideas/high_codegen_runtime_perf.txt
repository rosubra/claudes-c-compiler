Runtime Performance Improvements
=================================
Priority: HIGH

Remaining bottlenecks (profiled on zlib vs GCC -O2):

1. LOOP INDUCTION VARIABLE STRENGTH REDUCTION
   Array accesses in loops compute index*stride every iteration:
     movslq i, %rax; shlq $2, %rax; addq base, %rax; movl (%rax), ...
   Should increment a pointer instead: addq $4, %rdi; movl (%rdi), ...
   Fix: Loop strength reduction pass (iv_strength_reduce.rs exists but
   may need extension for pointer-based induction variables).

2. REDUNDANT SIGN EXTENSIONS
   The lowering emits Cast i32->i64 for array indices even when the
   value is already 64-bit. Codegen emits redundant movslq/cltq.
   The peephole catches some but not all cases.
   Fix: Track value widths in the IR and eliminate redundant casts.

3. REDUNDANT REGISTER-REGISTER MOVES
   Patterns like: movq %rax, %r14; movq %r14, %r15
   Arise because codegen routes through %rax as accumulator, then
   copies to callee-saved registers.
   Fix: Better peephole patterns, or teach the register allocator
   to avoid the accumulator roundtrip for register-to-register ops.

Key files:
- src/backend/regalloc.rs
- src/backend/liveness.rs
- src/backend/x86/codegen/peephole/
- src/passes/simplify.rs
- src/passes/iv_strength_reduce.rs
