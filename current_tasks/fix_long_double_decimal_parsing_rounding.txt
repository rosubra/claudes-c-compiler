Fix long double decimal-to-f128 parsing: implement rounding instead of truncation

Problem: decimal_to_float_bigint_f128() uses integer division which truncates,
causing values at exact powers of 2 (like LDBL_MIN = 2^(-16382)) to be parsed
as the largest subnormal instead of the smallest normal.

Example: "3.36210314311209350626267781732175260e-4932" should parse to
biased_exp=1, mantissa=0x10000000000000000000000000000 (f128 normal)
but instead parses to biased_exp=0 (f128 subnormal).

Root cause: The big integer division floors, losing the fractional remainder.
When the exact value is very close to (but slightly below) a power of 2,
the floored mantissa has all bits set (0x1ffff...ffff) with binary_exp one
too low, instead of rounding up to 0x10000...0000 with correct binary_exp.

Fix: After integer division, check the remainder against half the divisor
(round-to-nearest-even). If remainder > divisor/2, increment the mantissa.
This may carry into the exponent.

Also update __LDBL_MIN__ predefined macro to use GCC-matching full precision string.

Started: 2026-02-05
