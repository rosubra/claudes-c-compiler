Fix constant folding of width-sensitive binary operations (LShr, UDiv, URem)

Problem:
fold_binop() in constant_fold.rs doesn't know the IR type width.
For LShr, UDiv, URem, it always operates on i64/u64, but when the
IR type is I32/U32 (or narrower), the upper 32 bits of the sign-extended
i64 representation corrupt the result.

Example: __builtin_clrsb(-1) returns -1 instead of 31
- clrsb lowers to: LShr(x, 31), Neg, Xor, Clz, Sub(1)
- fold_binop(LShr, -1i64, 31) does (-1 as u64).wrapping_shr(31) = 0x1FFFFFFFF
- But for I32 type, should be (-1 as u32 as u64).wrapping_shr(31) = 1
- The wrong intermediate value cascades through the rest of the computation

Fix:
Pass the IR type to fold_binop. For LShr, UDiv, URem with 32-bit types,
mask/cast the operands to u32 before the operation.

Also fix static initializer string pointer arithmetic ("foo" + 1 in
static const arrays gives NULL pointer).

Status: in_progress
